{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a19cd62-7df3-40ac-b7e7-25821f94177c",
   "metadata": {},
   "source": [
    "# Quick Start Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45b8ff-3888-4f57-828c-98c9d5b3d361",
   "metadata": {},
   "source": [
    "A clean notebook for starting new analysis, which includes some findings from initial work (e.g. short membrane time constants and the option to implement Dale's Law). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c900baf-c033-4df4-a3db-71d4d25398b8",
   "metadata": {},
   "source": [
    "TODO:\n",
    "* Add a few lines of doccumentation per function (Inputs and outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a636b-f95e-4034-99da-ae958d253cb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2677e0-f786-4222-b36b-716b2cf8d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import matplotlib.cm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm.auto import tqdm as pbar\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ca855-9ec7-43c1-a0a3-3bee04e601e4",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bffae9-6755-4c52-ae31-91e8cee36c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SECONDS = 1\n",
    "MS = 1e-3\n",
    "HZ = 1\n",
    "\n",
    "DT = 1 * MS            # large time step to make simulations run faster\n",
    "ANF_PER_EAR = 100    # repeats of each ear with independent noise\n",
    "\n",
    "DURATION = .1 * SECONDS # stimulus duration\n",
    "DURATION_STEPS = int(np.round(DURATION / DT))\n",
    "INPUT_SIZE = 2 * ANF_PER_EAR\n",
    "\n",
    "# Training \n",
    "LR = 0.01\n",
    "N_EPOCHS = 50\n",
    "batch_size = 64\n",
    "n_training_batches = 64\n",
    "n_testing_batches = 32\n",
    "num_samples = batch_size*n_training_batches\n",
    "\n",
    "# classes at 15 degree increments\n",
    "NUM_CLASSES = 180 // 15\n",
    "print(f'Number of classes = {NUM_CLASSES}')\n",
    "\n",
    "# Network\n",
    "NUM_HIDDEN = 30 # number of hidden units\n",
    "TAU = 5 # membrane time constant\n",
    "IE_RATIO = 0.5 # ratio of inhibitory:excitatory units (used if DALES_LAW = True). 0 = all excitatory, 1 = all inhibitory \n",
    "DALES_LAW = False # When True, units will be only excitatory or inhibitory. When False, units will use both (like a normal ANN) \n",
    "if DALES_LAW:\n",
    "    print('Using Dales Law')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c55ffa2-b4c8-473b-8907-54138075df3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f838d0c-4ac1-4296-a2d2-abb278644142",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d168c57-5ae0-49ae-b510-555c612bdf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_signal(ipd):\n",
    "    \"\"\"\n",
    "    Generate an input signal (spike array) from array of true IPDs\n",
    "    \"\"\"\n",
    "    envelope_power = 2   # higher values make sharper envelopes, easier\n",
    "    rate_max = 600 * HZ   # maximum Poisson firing rate\n",
    "    stimulus_frequency = 20 * HZ\n",
    "\n",
    "    num_samples = len(ipd)\n",
    "    times = np.arange(DURATION_STEPS) * DT # array of times\n",
    "    phi = 2*np.pi*(stimulus_frequency * times + np.random.rand()) # array of phases corresponding to those times with random offset\n",
    "    # each point in the array will have a different phase based on which ear it is\n",
    "    # and its delay\n",
    "    theta = np.zeros((num_samples, DURATION_STEPS, 2*ANF_PER_EAR))\n",
    "    # for each ear, we have anf_per_ear different phase delays from to pi/2 so\n",
    "    # that the differences between the two ears can cover the full range from -pi/2 to pi/2\n",
    "    phase_delays = np.linspace(0, np.pi/2, ANF_PER_EAR)\n",
    "    # now we set up these theta to implement that. Some numpy vectorisation logic here which looks a little weird,\n",
    "    # but implements the idea in the text above.\n",
    "    theta[:, :, :ANF_PER_EAR] = phi[np.newaxis, :, np.newaxis]+phase_delays[np.newaxis, np.newaxis, :]\n",
    "    theta[:, :, ANF_PER_EAR:] = phi[np.newaxis, :, np.newaxis]+phase_delays[np.newaxis, np.newaxis, :]+ipd[:, np.newaxis, np.newaxis]\n",
    "    # now generate Poisson spikes at the given firing rate as in the previous notebook\n",
    "    spikes = np.random.rand(num_samples, DURATION_STEPS, 2*ANF_PER_EAR)<rate_max*DT*(0.5*(1+np.sin(theta)))**envelope_power\n",
    "    return spikes\n",
    "\n",
    "def random_ipd_input_signal(num_samples, tensor=True):\n",
    "    \"\"\"\n",
    "    Generate the training data\n",
    "    Returns true IPDs from U(-pi/2, pi/2) and corresponding spike arrays\n",
    "    \"\"\"\n",
    "    ipd = np.random.rand(num_samples)*np.pi-np.pi/2 # uniformly random in (-pi/2, pi/2)\n",
    "    spikes = spikes_from_fixed_idp_input_signal(ipd, tensor)\n",
    "\n",
    "    if tensor:\n",
    "        ipd = torch.tensor(ipd, device=device, dtype=dtype)        \n",
    "\n",
    "    return ipd, spikes\n",
    "\n",
    "def spikes_from_fixed_idp_input_signal(ipd, tensor=True):\n",
    "    spikes = input_signal(ipd)\n",
    "    if tensor:\n",
    "        spikes = torch.tensor(spikes, device=device, dtype=dtype)\n",
    "    return spikes\n",
    "\n",
    "def show_examples(shown=8):\n",
    "    ipd = np.linspace(-np.pi/2, np.pi/2, shown)\n",
    "    spikes = spikes_from_fixed_idp_input_signal(ipd, shown).cpu()\n",
    "\n",
    "    plt.figure(figsize=(10, 4), dpi=100)\n",
    "    for i in range(shown):\n",
    "        plt.subplot(2, shown // 2, i+1)\n",
    "        plt.imshow(spikes[i, :, :].T, aspect='auto', interpolation='nearest', cmap=plt.cm.gray_r)\n",
    "        plt.title(f'True IPD = {int(ipd[i]*180/np.pi)} deg')\n",
    "        if i>=4:\n",
    "            plt.xlabel('Time (steps)')\n",
    "        if i%4==0:\n",
    "            plt.ylabel('Input neuron index')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def data_generator(ipds, spikes):\n",
    "    perm = torch.randperm(spikes.shape[0])\n",
    "    spikes = spikes[perm, :, :]\n",
    "    ipds = ipds[perm]\n",
    "    n, _, _ = spikes.shape\n",
    "    n_batch = n//batch_size\n",
    "    for i in range(n_batch):\n",
    "        x_local = spikes[i*batch_size:(i+1)*batch_size, :, :]\n",
    "        y_local = ipds[i*batch_size:(i+1)*batch_size]\n",
    "        yield x_local, y_local\n",
    "        \n",
    "def discretise(ipds):\n",
    "    return ((ipds+np.pi/2) * NUM_CLASSES / np.pi).long() # assumes input is tensor\n",
    "\n",
    "def continuise(ipd_indices): # convert indices back to IPD midpoints\n",
    "    return (ipd_indices+0.5) / NUM_CLASSES * np.pi - np.pi / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd60321-1802-471e-87c9-083282a1df73",
   "metadata": {},
   "source": [
    "### SNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6ae7e-8695-43d6-8db1-9f5c01024df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, beta):\n",
    "    return 1 / (1 + torch.exp(-beta*x))\n",
    "\n",
    "def sigmoid_deriv(x, beta):\n",
    "    s = sigmoid(x, beta)\n",
    "    return beta * s * (1 - s)\n",
    "\n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inp):\n",
    "        ctx.save_for_backward(inp)\n",
    "        out = torch.zeros_like(inp)\n",
    "        out[inp > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        inp, = ctx.saved_tensors\n",
    "        sigmoid_derivative = sigmoid_deriv(inp, beta=5)\n",
    "        grad = grad_output*sigmoid_derivative\n",
    "        return grad\n",
    "\n",
    "spike_fn  = SurrGradSpike.apply\n",
    "\n",
    "def membrane_only(input_spikes, weights, tau):\n",
    "    \"\"\"\n",
    "    :param input_spikes: has shape (batch_size, duration_steps, input_size)\n",
    "    :param weights: has shape  (input_size, num_classes\n",
    "    :param tau: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    batch_size = input_spikes.shape[0]\n",
    "    assert len(input_spikes.shape) == 3\n",
    "    \n",
    "    v = torch.zeros((batch_size, NUM_CLASSES), device=device, dtype=dtype)\n",
    "    v_rec = [v]\n",
    "    h = torch.einsum(\"abc,cd->abd\", (input_spikes, weights))\n",
    "    alpha = np.exp(-DT / tau)\n",
    "    for t in range(DURATION_STEPS - 1):\n",
    "        v = alpha*v + h[:, t, :]\n",
    "        v_rec.append(v)\n",
    "    v_rec = torch.stack(v_rec, dim=1)  # (batch_size, duration_steps, num_classes)\n",
    "    return v_rec\n",
    "\n",
    "def layer1(input_spikes, w1, tau, sign1):\n",
    "    \n",
    "    if DALES_LAW:\n",
    "        w1 = get_signed_weights(w1, sign1)\n",
    "\n",
    "    batch_size = input_spikes.shape[0]\n",
    "\n",
    "    # First layer: input to hidden\n",
    "    v = torch.zeros((batch_size, NUM_HIDDEN), device=device, dtype=dtype)\n",
    "    s = torch.zeros((batch_size, NUM_HIDDEN), device=device, dtype=dtype)\n",
    "    s_rec = [s]\n",
    "    h = torch.einsum(\"abc,cd->abd\", (input_spikes, w1))\n",
    "    alpha = np.exp(-DT / tau)\n",
    "    \n",
    "    for t in range(DURATION_STEPS - 1):\n",
    "        new_v = (alpha*v + h[:, t, :])*(1-s) # multiply by 0 after a spike\n",
    "        s = spike_fn(v-1) # threshold of 1\n",
    "        v = new_v\n",
    "        s_rec.append(s)\n",
    "    s_rec = torch.stack(s_rec, dim=1)\n",
    "    return s_rec\n",
    "\n",
    "def layer2(s_rec, w2, tau, sign2):\n",
    "    \"\"\"Second layer: hidden to output\"\"\"\n",
    "    if DALES_LAW:\n",
    "        w2 = get_signed_weights(w2, sign2)\n",
    "\n",
    "    v_rec = membrane_only(s_rec, w2, tau=tau)\n",
    "    return v_rec\n",
    "\n",
    "def snn(input_spikes, w1, w2, signs, tau=5*MS):\n",
    "    \"\"\"Run the simulation\"\"\"\n",
    "        \n",
    "    s_rec = layer1(input_spikes, w1, tau, signs[0])\n",
    "    v_rec = layer2(s_rec, w2, tau, signs[1])\n",
    "\n",
    "    # Return recorded membrane potential of output\n",
    "    return v_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706d488-162c-4e70-86cd-340da7d49115",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dale's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e088884-247b-4d38-a6bf-15c8cd9e1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dales_mask(nb_inputs, nb_out, ie_ratio) : \n",
    "\n",
    "    d_mask = torch.ones(nb_inputs, nb_out)\n",
    "    #inhib_units = np.random.choice(nb_inputs, int(nb_inputs*ie_ratio), replace=False)\n",
    "    inhib_units = torch.arange(ie_ratio*nb_inputs, dtype=int)\n",
    "    d_mask[inhib_units, :] = -1\n",
    "    return d_mask\n",
    "\n",
    "def init_weight_matrices(ie_ratio = 0.1):\n",
    "    \"\"\"Weights and uniform weight initialisation\"\"\"\n",
    "\n",
    "    # Input to hidden layer\n",
    "    w1 = nn.Parameter(torch.empty((INPUT_SIZE, NUM_HIDDEN), device=device, dtype=dtype, requires_grad=True))\n",
    "    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(w1)\n",
    "    bound = 1 / np.sqrt(fan_in)\n",
    "    nn.init.uniform_(w1, -bound, bound)\n",
    "\n",
    "    # Hidden layer to output\n",
    "    w2 = nn.Parameter(torch.empty((NUM_HIDDEN, NUM_CLASSES), device=device, dtype=dtype, requires_grad=True))\n",
    "    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(w2)\n",
    "    bound = 1 / np.sqrt(fan_in)\n",
    "    nn.init.uniform_(w2, -bound, bound)\n",
    "\n",
    "    #Get fixed signs for the weight, 90% excitatory \n",
    "    signs = [get_dales_mask(*w.shape, ie_ratio).to(w.device) for w in (w1, w2)]\n",
    "\n",
    "    return w1, w2, signs\n",
    "\n",
    "def get_signed_weights(w, sign):\n",
    "    \"\"\"Get the signed value of the weight\"\"\"\n",
    "    # Note abs is in principle not differentiable.\n",
    "    # In practice, pytorch will set the derivative to 0 when the values are 0.\n",
    "    # (see https://discuss.pytorch.org/t/how-does-autograd-deal-with-non-differentiable-opponents-such-as-abs-and-max/34538)\n",
    "    # This has the adverse effect that, during training, if a synapse reaches 0,\n",
    "    # it is \"culled\" and can not be recovered.\n",
    "    # It should be possible to cheat here and either \"wiggle\" 0-valued synapses,\n",
    "    # or to override abs gradient to return a very small random number.\n",
    "\n",
    "    #TODO try ReLu or other activation\n",
    "    #TODO reproduce paper https://www.biorxiv.org/content/10.1101/2020.11.02.364968v2.full\n",
    "\n",
    "   # return torch.max(w, 0)*sign\n",
    "    return torch.abs(w)*sign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c215cfc6-d04d-4ad4-8314-a7c627939ff7",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c7c066-4bc4-4604-8a40-6989b57f19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(w1, w2, signs, ipds, spikes, ipds_validation, spikes_validation, lr=0.01, n_epochs=30, tau=5*MS):\n",
    "    \"\"\"\n",
    "    :param lr: learning rate\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Optimiser and loss function\n",
    "    optimizer = torch.optim.Adam([w1, w2], lr=lr)\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "\n",
    "    loss_hist = []\n",
    "    val_loss_hist = []\n",
    "\n",
    "    best_loss = 1e10\n",
    "    val_loss_best_loss = 1e10\n",
    "\n",
    "    for e in pbar(range(n_epochs)):\n",
    "        local_loss = []\n",
    "        for x_local, y_local in data_generator(discretise(ipds), spikes):\n",
    "            # Run network\n",
    "            output = snn(x_local, w1, w2, signs, tau=tau)\n",
    "            # Compute cross entropy loss\n",
    "            m = torch.sum(output, 1)*0.01  # Sum time dimension\n",
    "\n",
    "            reg = 0\n",
    "            loss = loss_fn(log_softmax_fn(m), y_local) + reg\n",
    "            local_loss.append(loss.item())\n",
    "\n",
    "            # Update gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_hist.append(np.mean(local_loss))\n",
    "        \n",
    "        val_local_loss = []\n",
    "        for x_local, y_local in data_generator(discretise(ipds_validation), spikes_validation):\n",
    "            # Run network\n",
    "            output = snn(x_local, w1, w2, signs, tau=tau)\n",
    "            # Compute cross entropy loss\n",
    "            m = torch.sum(output, 1)*0.01  # Sum time dimension\n",
    "\n",
    "            val_loss = loss_fn(log_softmax_fn(m), y_local) \n",
    "            val_local_loss.append(val_loss.item())\n",
    "\n",
    "        val_loss_hist.append(np.mean(val_local_loss))\n",
    "\n",
    "        if np.mean(val_local_loss) < val_loss_best_loss:\n",
    "            val_loss_best_loss = np.mean(val_local_loss)\n",
    "            if DALES_LAW:\n",
    "                best_weights = get_signed_weights(w1, signs[0]), get_signed_weights(w2, signs[1]), signs\n",
    "            else:\n",
    "                best_weights = w1, w2, signs\n",
    "\n",
    "        #Early Stopping : \n",
    "        if torch.tensor(val_loss_hist[-10:]).argmin() == 0  and e>10: \n",
    "            print('Early Stop !')\n",
    "            return best_weights\n",
    "\n",
    "    # Plot the loss function over time\n",
    "    plt.plot(loss_hist)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.plot(val_loss_hist)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if DALES_LAW:\n",
    "        return get_signed_weights(w1, signs[0]), get_signed_weights(w2, signs[1]), signs\n",
    "    else:\n",
    "        return w1, w2, signs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263eb0bc-edf4-4e8d-80d5-55b98889b0c6",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6877e75a-3e5c-4a95-9c2c-7fb3caf608a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(ipds, spikes, run):\n",
    "    accs = []\n",
    "    ipd_true = []\n",
    "    ipd_est = []\n",
    "    confusion = np.zeros((NUM_CLASSES, NUM_CLASSES))\n",
    "    for x_local, y_local in data_generator(ipds, spikes):\n",
    "        y_local_orig = y_local\n",
    "        y_local = discretise(y_local)\n",
    "        output = run(x_local)\n",
    "        m = torch.sum(output, 1)  # Sum time dimension\n",
    "        _, am = torch.max(m, 1)  # argmax over output units\n",
    "        tmp = np.mean((y_local == am).detach().cpu().numpy())  # compare to labels\n",
    "        for i, j in zip(y_local.detach().cpu().numpy(), am.detach().cpu().numpy()):\n",
    "            confusion[j, i] += 1\n",
    "        ipd_true.append(y_local_orig.cpu().data.numpy())\n",
    "        ipd_est.append(continuise(am.detach().cpu().numpy()))\n",
    "        accs.append(tmp)\n",
    "\n",
    "    ipd_true = np.hstack(ipd_true)\n",
    "    ipd_est = np.hstack(ipd_est)\n",
    "\n",
    "    return ipd_true, ipd_est, confusion, accs\n",
    "\n",
    "def report_accuracy(ipd_true, ipd_est, confusion, accs, label):\n",
    "\n",
    "    abs_errors_deg = abs(ipd_true-ipd_est)*180/np.pi\n",
    "\n",
    "    print()\n",
    "    print(f\"{label} classifier accuracy: {100*np.mean(accs):.1f}%\")\n",
    "    print(f\"{label} absolute error: {np.mean(abs_errors_deg):.1f} deg\")\n",
    "\n",
    "    plt.figure(figsize=(10, 4), dpi=100)\n",
    "    plt.subplot(121)\n",
    "    plt.hist(ipd_true * 180 / np.pi, bins=NUM_CLASSES, label='True')\n",
    "    plt.hist(ipd_est * 180 / np.pi, bins=NUM_CLASSES, label='Estimated')\n",
    "    plt.xlabel(\"IPD\")\n",
    "    plt.yticks([])\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(label)\n",
    "    plt.subplot(122)\n",
    "    confusion /= np.sum(confusion, axis=0)[np.newaxis, :]\n",
    "    plt.imshow(confusion, interpolation='nearest', aspect='equal', origin='lower', extent=(-90, 90, -90, 90))\n",
    "    plt.xlabel('True IPD')\n",
    "    plt.ylabel('Estimated IPD')\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.tight_layout()    \n",
    "\n",
    "def analyse_accuracy(ipds, spikes, run, label):\n",
    "    ipd_true, ipd_est, confusion, accs = test_accuracy(ipds, spikes, run)\n",
    "    report_accuracy(ipd_true, ipd_est, confusion, accs, label)\n",
    "    return 100*np.mean(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4de8bd-0ea7-4f5b-a2d5-209f37755916",
   "metadata": {},
   "source": [
    "## Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95d56c-7e14-4d17-9b96-acd3daa342cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training data\n",
    "w1, w2, signs = init_weight_matrices(ie_ratio=IE_RATIO)\n",
    "\n",
    "ipds_training, spikes_training = random_ipd_input_signal(num_samples)\n",
    "ipds_validation, spikes_validation = random_ipd_input_signal(num_samples)\n",
    "\n",
    "# Train network\n",
    "w1_trained, w2_trained, signs = train(w1, w2, signs, ipds_training, spikes_training, ipds_validation, spikes_validation, lr=LR, n_epochs=N_EPOCHS, tau=TAU*MS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf0fa2-31f0-41e7-91bb-284396da2e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse\n",
    "print(f\"Chance accuracy level: {100 * 1 / NUM_CLASSES:.1f}%\")\n",
    "run_func = lambda x: snn(x, w1_trained, w2_trained, signs)\n",
    "analyse_accuracy(ipds_training, spikes_training, run_func, 'Train')\n",
    "\n",
    "ipds_test, spikes_test = random_ipd_input_signal(batch_size*n_testing_batches)\n",
    "analyse_accuracy(ipds_test, spikes_test, run_func, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2f102-8569-492c-8e61-50c4cb59ac68",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b3939c-ec09-4d42-b6f6-0dc39768751e",
   "metadata": {},
   "source": [
    "Add your own analysis here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
